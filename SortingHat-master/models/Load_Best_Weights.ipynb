{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "np.random.seed(512)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "max_features = 128\n",
    "maxlen = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (2,5,10,11,12,13,14,15,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "dict_label = {'Usable directly numeric':0, 'Usable with extraction':1, 'Usable with Extration': 1, 'Usable with extraction ':1, 'Usable directly categorical':2, 'Unusable':3, 'Context_specific':4, 'Usable directly categorical ':2}\n",
    "data = pd.read_csv('data_for_ML_num.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]\n",
    "data_LSTM = pd.concat([data['Attribute_name'], data['sample_1'], data['sample_2'], data['sample_3'], data['sample_4'], data['sample_5']], axis =1)\n",
    "\n",
    "# X_train, X_test,y_train,y_test = train_test_split(data,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hghj\n",
      "hghj\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'Num of nans': 'Num_of_nans', 'num of dist_val': 'num_of_dist_val'})\n",
    "\n",
    "data['Num_of_nans'] = [float(data['Num_of_nans'][i])/float(data['Total_val'][i]) for i in data.index]\n",
    "data['num_of_dist_val'] = [float(data['num_of_dist_val'][i])/float(data['Total_val'][i]) for i in data.index]\n",
    "\n",
    "data1 = data[['Num_of_nans', 'max_val', 'mean', 'min_val', 'num_of_dist_val','std_dev','castability','extractability', 'len_val']]\n",
    "data1 = data1.fillna(0)\n",
    "print('hghj')\n",
    "data1 = data1.rename(columns={'mean': 'scaled_mean', 'min_val': 'scaled_min_val', 'max_val': 'scaled_max_val','std_dev': 'scaled_std_dev'})\n",
    "data1.loc[data1['scaled_min_val'] > 10000, 'scaled_min_val'] = 10000\n",
    "data1.loc[data1['scaled_min_val'] < -10000, 'scaled_min_val'] = -10000\n",
    "data1.loc[data1['scaled_max_val'] > 10000, 'scaled_max_val'] = 10000\n",
    "data1.loc[data1['scaled_max_val'] < -10000, 'scaled_max_val'] = -10000\n",
    "data1.loc[data1['scaled_mean'] > 10000, 'scaled_mean'] = 10000\n",
    "data1.loc[data1['scaled_mean'] < -10000, 'scaled_mean'] = -10000\n",
    "data1.loc[data1['scaled_std_dev'] > 10000, 'scaled_std_dev'] = 10000\n",
    "data1.loc[data1['scaled_std_dev'] < -10000, 'scaled_std_dev'] = -10000\n",
    "column_names_to_normalize = ['scaled_max_val', 'scaled_mean', 'scaled_min_val','scaled_std_dev']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "data1.Num_of_nans = data1.Num_of_nans.astype(float)\n",
    "data1.num_of_dist_val = data1.num_of_dist_val.astype(float)\n",
    "data1.castability = data1.castability.astype(float)\n",
    "data1.extractability = data1.extractability.astype(float)\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "# import enchant\n",
    "# data1.to_csv('before.csv')\n",
    "# f = open('current.txt','w')\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "\n",
    "# for i in data.index:\n",
    "#     ival = data.at[i,'Attribute_name']\n",
    "#     if ival != 'id' and d.check(ival):\n",
    "#         print >> f,ival\n",
    "#         print >> f,y.at[i,'y_act']\n",
    "#         data1.at[i,'dictionary_item'] = 1\n",
    "#     else:\n",
    "#         data1.at[i,'dictionary_item'] = 0\n",
    "\n",
    "# data1.to_csv('after.csv')\n",
    "# f.close()\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# arr = data['Attribute_name'].values\n",
    "# data = data.fillna(0)\n",
    "# arr1 = data['sample_1'].values\n",
    "# arr1 = [str(x) for x in arr1]\n",
    "# arr2 = data['sample_2'].values\n",
    "# arr2 = [str(x) for x in arr2]\n",
    "# arr3 = data['sample_3'].values\n",
    "# arr3 = [str(x) for x in arr3]\n",
    "\n",
    "# # print(arr)\n",
    "# # print(arr1)\n",
    "# vectorizer = CountVectorizer(ngram_range=(3,3),analyzer='char')\n",
    "# X = vectorizer.fit_transform(arr)\n",
    "# X1 = vectorizer.fit_transform(arr1)\n",
    "# X2 = vectorizer.fit_transform(arr2)\n",
    "# X3 = vectorizer.fit_transform(arr3)\n",
    "\n",
    "# print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "# data1.to_csv('before.csv')\n",
    "# tempdf = pd.DataFrame(X.toarray())\n",
    "# tempdf1 = pd.DataFrame(X1.toarray())\n",
    "# tempdf2 = pd.DataFrame(X2.toarray())\n",
    "# tempdf3 = pd.DataFrame(X3.toarray())\n",
    "\n",
    "# data2 = pd.concat([data1,tempdf], axis=1, sort=False)\n",
    "# data2.to_csv('after.csv')\n",
    "\n",
    "\n",
    "print('hghj')\n",
    "X_train, X_test,y_train,y_test = train_test_split(data1,y, test_size=0.2,random_state=100)\n",
    "\n",
    "key_name = data['Attribute_name']\n",
    "atr_train,atr_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "# print(atr_train)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "atr_train.reset_index(inplace=True,drop=True)\n",
    "atr_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "list_sentences_train = atr_train.values\n",
    "list_sentences_test = atr_test.values\n",
    "\n",
    "\n",
    "structured_data_train = X_train \n",
    "structured_data_test = X_test\n",
    "\n",
    "\n",
    "# structured_data_train = X_train.loc[:,['Num_of_nans', 'scaled_max_val', 'scaled_mean', 'scaled_min_val', 'num_of_dist_val','scaled_std_dev','castability','extractability', 'len_val']]\n",
    "# structured_data_test = X_test.loc[:,['Num_of_nans', 'scaled_max_val', 'scaled_mean', 'scaled_min_val', 'num_of_dist_val','scaled_std_dev','castability','extractability', 'len_val']]\n",
    "\n",
    "# structured_data_train = structured_data_train.reset_index(drop=True)\n",
    "# structured_data_test = structured_data_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "structured_input_train = []\n",
    "for i in range(len(structured_data_train)):\n",
    "#     print(structured_data_train.loc[i])\n",
    "#     print(list(structured_data_train.loc[i]))\n",
    "    structured_input_train.append(list(structured_data_train.loc[i]))\n",
    "structured_input_train = np.array(structured_input_train).reshape(len(structured_data_train),len(structured_data_train.keys()))    \n",
    "\n",
    "structured_input_test = []\n",
    "for i in range(len(structured_data_train)):\n",
    "    structured_input_test.append(list(structured_data_train.loc[i]))\n",
    "structured_input_test = np.array(structured_input_test).reshape(len(structured_data_train),len(structured_data_train.keys()))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_name = data['sample_1']\n",
    "samp1_train,samp1_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "samp1_train.reset_index(inplace=True,drop=True)\n",
    "samp1_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "key_name = data['sample_2']\n",
    "samp2_train,samp2_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "samp2_train.reset_index(inplace=True,drop=True)\n",
    "samp2_test.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#NULL!' '0' '268' ... 'Hate' '-0.101' '2']\n",
      "['0' '78' '110' ... nan '0.171' '5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_sentences_train = atr_train.values\n",
    "list_sentences_test = atr_test.values\n",
    "\n",
    "# X_train.sample_1 = X_train.sample_1.astype(str)\n",
    "# X_test.sample_1 = X_test.sample_1.astype(str)\n",
    "\n",
    "list_sentences_train1 = samp1_train.values\n",
    "list_sentences_test1 = samp1_test.values\n",
    "\n",
    "print(list_sentences_train1)\n",
    "\n",
    "# X_train.sample_2 = X_train.sample_2.astype(str)\n",
    "# X_test.sample_2 = X_test.sample_2.astype(str)\n",
    "\n",
    "list_sentences_train2 = samp2_train.values\n",
    "list_sentences_test2 = samp2_test.values\n",
    "\n",
    "print(list_sentences_train2)\n",
    "\n",
    "for i in range(len(list_sentences_train)):\n",
    "    list_sentences_train[i] = str(list_sentences_train[i])\n",
    "    list_sentences_train1[i] = str(list_sentences_train1[i])\n",
    "    list_sentences_train2[i] = str(list_sentences_train2[i])\n",
    "    \n",
    "for i in range(len(list_sentences_test)):\n",
    "    list_sentences_test[i] = str(list_sentences_test[i])\n",
    "    list_sentences_test1[i] = str(list_sentences_test1[i])\n",
    "    list_sentences_test2[i] = str(list_sentences_test2[i])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "tokenizer1 = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer1.fit_on_texts(list(list_sentences_train1))\n",
    "# train data\n",
    "list_tokenized_train1 = tokenizer.texts_to_sequences(list_sentences_train1)\n",
    "X_t1 = keras_seq.pad_sequences(list_tokenized_train1, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test1 = tokenizer.texts_to_sequences(list_sentences_test1)\n",
    "X_te1 = keras_seq.pad_sequences(list_tokenized_test1, maxlen=maxlen)\n",
    "\n",
    "\n",
    "tokenizer2 = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer2.fit_on_texts(list(list_sentences_train1))\n",
    "# train data\n",
    "list_tokenized_train2 = tokenizer.texts_to_sequences(list_sentences_train2)\n",
    "X_t2 = keras_seq.pad_sequences(list_tokenized_train2, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test2 = tokenizer.texts_to_sequences(list_sentences_test2)\n",
    "X_te2 = keras_seq.pad_sequences(list_tokenized_test2, maxlen=maxlen)\n",
    "\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "7400/7400 [==============================] - 1s 96us/step\n",
      "(0.050643856878568594, 0.9816216216216216)\n",
      "1851/1851 [==============================] - 0s 68us/step\n",
      "(0.5080923764513094, 0.8995137766591282)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bestone = load_model('best_weights.h5')\n",
    "print('---')\n",
    "loss, acc = bestone.evaluate([X_t,X_t1,structured_data_train],to_categorical(y_train),verbose=1)\n",
    "print(loss, acc)\n",
    "loss, acc = bestone.evaluate([X_te,X_te1,structured_data_test],to_categorical(y_test),verbose=1)\n",
    "print(loss, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
