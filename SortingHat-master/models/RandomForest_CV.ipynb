{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "# import enchant\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# import graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "# from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import enchant\n",
    "np.random.seed(512)\n",
    "# import subprocess\n",
    "# subprocess.call([\"pip\",\"install\", \"pyenchant\",\"--\",\"user\"])\n",
    "# os.system('pip install pyenchant')\n",
    "# import nltk\n",
    "# from nltk.corpus import words\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (2,5,10,11,12,13,14,15,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "dict_label = {'Usable directly numeric':0, 'Usable with extraction':1, 'Usable with Extration': 1, 'Usable with extraction ':1, 'Usable directly categorical':2, 'Unusable':3, 'Context_specific':4, 'Usable directly categorical ':2}\n",
    "data = pd.read_csv('data_for_ML_num.csv')\n",
    "\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Num of nans'] = [data['Num of nans'][i]*100/data['Total_val'][i] for i in data.index]\n",
    "data['num of dist_val'] = [data['num of dist_val'][i]*100/data['Total_val'][i] for i in data.index]\n",
    "\n",
    "data1 = data[['Num of nans', 'max_val', 'mean', 'min_val', 'num of dist_val','std_dev','castability','extractability', 'len_val']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "# data = data.rename(columns={'mean': 'scaled_mean', 'min_val': 'scaled_min_val', 'max_val': 'scaled_max_val','std_dev': 'scaled_std_dev','len_val': 'scaled_len_val'})\n",
    "# column_names_to_normalize = ['scaled_max_val', 'scaled_mean', 'scaled_min_val','scaled_std_dev', 'scaled_len_val']\n",
    "# x = data[column_names_to_normalize].values\n",
    "# x = np.nan_to_num(x)\n",
    "# x_scaled = StandardScaler().fit_transform(x)\n",
    "# df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data.index)\n",
    "# data[column_names_to_normalize] = df_temp\n",
    "\n",
    "# X_train, X_test,y_train,y_test = train_test_split(data,y, test_size=0.2)\n",
    "\n",
    "# new_data = data[['scaled_min_val']]\n",
    "# new_data.scaled_min_val = new_data.scaled_min_val.astype(float)\n",
    "# normalized_df=(new_data-new_data.mean())/new_data.std()\n",
    "# new_data = new_data[np.abs(new_data.scaled_min_val-new_data.scaled_min_val.mean()) <= (3*new_data.scaled_min_val.std())]\n",
    "\n",
    "# for index, row in new_data.iterrows():\n",
    "#     if row['scaled_min_val'] > 100000:\n",
    "#         new_data.iloc[index]['scaled_min_val'] = 100000\n",
    "#     if row['scaled_min_val'] < -100000:\n",
    "#         new_data.iloc[index]['scaled_min_val'] = -100000        \n",
    "#     print(row['scaled_min_val']) \n",
    "\n",
    "# q = new_data['scaled_min_val'].quantile(0.99)\n",
    "# new_data = new_data[new_data['scaled_min_val'] < q]\n",
    "# print(new_data)\n",
    "\n",
    "# print(new_data.mean())\n",
    "# print(new_data.std())\n",
    "\n",
    "\n",
    "# scaled_features = StandardScaler().fit_transform(new_data.values)\n",
    "# df = pd.DataFrame(scaled_features)\n",
    "# scaled_features = MinMaxScaler().fit_transform(new_data.values)\n",
    "# df = pd.DataFrame(scaled_features)\n",
    "\n",
    "# std_scale = preprocessing.StandardScaler().fit(data[['max_val', 'min_val']])\n",
    "# data = std_scale.transform(data[['max_val', 'min_val']])\n",
    "# data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1.to_csv('before.csv')\n",
    "# f = open('current.txt','w')\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "for i in data.index:\n",
    "    ival = data.at[i,'Attribute_name']\n",
    "    if ival != 'id' and d.check(ival):\n",
    "#         print >> f,ival\n",
    "#         print >> f,y.at[i,'y_act']\n",
    "        data1.at[i,'dictionary_item'] = 1\n",
    "    else:\n",
    "        data1.at[i,'dictionary_item'] = 0\n",
    "\n",
    "# data1.to_csv('after.csv')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286\n"
     ]
    }
   ],
   "source": [
    "# print(data1.columns)\n",
    "\n",
    "arr = data['Attribute_name'].values\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2),analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "# print(X.toarray())\n",
    "\n",
    "# data1.to_csv('before.csv')\n",
    "\n",
    "tempdf = pd.DataFrame(X.toarray())\n",
    "\n",
    "data2 = pd.concat([data1,tempdf], axis=1, sort=False)\n",
    "\n",
    "# data2.to_csv('after.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9677364864864865\n",
      "0.8912162162162162\n",
      "0.8978930307941653\n",
      "0.9675675675675676\n",
      "0.8695945945945946\n",
      "0.8833063209076175\n",
      "0.9721283783783784\n",
      "0.8858108108108108\n",
      "0.8914100486223663\n",
      "0.972804054054054\n",
      "0.8945945945945946\n",
      "0.8941112911939492\n",
      "0.9716216216216216\n",
      "0.8831081081081081\n",
      "0.8946515397082658\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(data2,y, test_size=0.2,random_state=100)\n",
    "print(len(X_train))\n",
    "data2 = data2.rename(columns={'mean': 'scaled_mean', 'min_val': 'scaled_min_val', 'max_val': 'scaled_max_val','std_dev': 'scaled_std_dev'})\n",
    "\n",
    "\n",
    "X_train_new = X_train.reset_index(drop=True)\n",
    "y_train_new = y_train.reset_index(drop=True)\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())\n",
    "\n",
    "X_train_new = X_train_new.values\n",
    "y_train_new = y_train_new.values\n",
    "# print(X_train_new)\n",
    "# print(y_train_new)\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "avg_train_acc,avg_test_acc = 0,0\n",
    "    \n",
    "n_estimators_grid = [5,25,50,75,100]\n",
    "max_depth_grid = [5,10,25,50,100]\n",
    "\n",
    "avgsc_lst,avgsc_train_lst,avgsc_hld_lst = [],[],[]\n",
    "avgsc,avgsc_train,avgsc_hld = 0,0,0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_new):\n",
    "    X_train_cur, X_test_cur = X_train_new[train_index], X_train_new[test_index]\n",
    "    y_train_cur, y_test_cur = y_train_new[train_index], y_train_new[test_index]\n",
    "    X_train_train, X_val,y_train_train,y_val = train_test_split(X_train_cur,y_train_cur, test_size=0.25,random_state=100)\n",
    "    \n",
    "    bestPerformingModel = RandomForestClassifier(n_estimators=10,max_depth=5)\n",
    "    bestscore = 0\n",
    "    for ne in n_estimators_grid:\n",
    "        for md in max_depth_grid:\n",
    "            clf = RandomForestClassifier(n_estimators=ne,max_depth=md)\n",
    "            clf.fit(X_train_train, y_train_train)\n",
    "            sc = clf.score(X_val, y_val)\n",
    "    \n",
    "            if bestscore < sc:\n",
    "                bestscore = sc\n",
    "                bestPerformingModel = clf\n",
    "#                 print(bestPerformingModel)\n",
    "\n",
    "    bscr_train = bestPerformingModel.score(X_train_cur, y_train_cur)\n",
    "    bscr = bestPerformingModel.score(X_test_cur, y_test_cur)\n",
    "    bscr_hld = bestPerformingModel.score(X_test, y_test)\n",
    "\n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    avgsc_lst.append(bscr)\n",
    "    avgsc_hld_lst.append(bscr_hld)\n",
    "    \n",
    "    avgsc_train = avgsc_train + bscr_train    \n",
    "    avgsc = avgsc + bscr\n",
    "    avgsc_hld = avgsc_hld + bscr_hld\n",
    "\n",
    "    print(bscr_train)\n",
    "    print(bscr)\n",
    "    print(bscr_hld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9677364864864865, 0.9675675675675676, 0.9721283783783784, 0.972804054054054, 0.9716216216216216]\n",
      "[0.8912162162162162, 0.8695945945945946, 0.8858108108108108, 0.8945945945945946, 0.8831081081081081]\n",
      "[0.8978930307941653, 0.8833063209076175, 0.8914100486223663, 0.8941112911939492, 0.8946515397082658]\n",
      "0.9703716216216216\n",
      "0.8848648648648648\n",
      "0.8922744462452729\n"
     ]
    }
   ],
   "source": [
    "print(avgsc_train_lst)\n",
    "print(avgsc_lst)\n",
    "print(avgsc_hld_lst)\n",
    "\n",
    "print(avgsc_train/k)\n",
    "print(avgsc/k)\n",
    "print(avgsc_hld/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
